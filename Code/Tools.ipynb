{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["LwULoKGq6a-R","-eUHSS9oFAno"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Set Up"],"metadata":{"id":"LwULoKGq6a-R"}},{"cell_type":"code","source":["from functools import partial\n","from pathlib import Path\n","import os, io, cv2, joblib, random, warnings, time\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib import patches\n","from collections import Counter\n","import seaborn as sns\n","from datetime import datetime\n","from PIL import Image\n","from sklearn.utils import resample\n","from imblearn.over_sampling import RandomOverSampler\n","import pickle\n","from skimage.feature import hog\n","from sklearn.decomposition import PCA\n","import zipfile\n","from skimage import io, color, img_as_ubyte, data, exposure, img_as_float\n","from sklearn.utils import shuffle\n","from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n","from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import RandomizedSearchCV\n","from sklearn.cluster import MiniBatchKMeans\n","import sklearn.svm as svm\n","from sklearn.preprocessing import LabelEncoder\n","\n","import keras_tuner\n","import keras\n","import matplotlib\n","from scikeras.wrappers import KerasClassifier\n","import tensorflow.keras as K\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Lambda, Dense, GlobalAveragePooling2D, Flatten, Dropout, Conv2D, MaxPooling2D, GlobalMaxPooling2D, BatchNormalization, Rescaling, AveragePooling2D, Input, Add, Activation\n","from tensorflow.keras.models import Model, load_model, Sequential\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler, History, ModelCheckpoint\n","\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.applications.resnet50 import preprocess_input as resnet50_preprocess\n","\n","from tensorflow.keras.applications import Xception\n","from tensorflow.keras.applications.xception import preprocess_input  as xception_preprocess\n","\n","from tensorflow.keras.applications import MobileNetV2\n","from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenet_preprocess\n","\n","from tensorflow.keras.applications import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input  as vgg16_preprocess\n","\n","from tensorflow.keras.applications import EfficientNetB0\n","from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preproces\n","\n","from tensorflow.keras.applications import InceptionV3\n","from tensorflow.keras.applications.inception_v3 import preprocess_input as inceptionv3_preprocess\n","\n","\n","from kerastuner import HyperModel, RandomSearch\n","\n","from matplotlib import rc\n","import matplotlib.animation as animation\n","\n","rc('animation', html='jshtml')\n","\n","plt.rcParams['animation.embed_limit'] = 50.0\n","\n","import logging\n","logging.disable(logging.WARNING)\n","\n","%matplotlib inline\n","%load_ext autotime"],"metadata":{"id":"7GnrRo2cgoqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q2BrGoczEm7l","executionInfo":{"status":"ok","timestamp":1711968259464,"user_tz":-60,"elapsed":4,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0f8c43bc-69eb-4602-e217-c58de8c6aa0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 985 µs (started: 2024-04-01 10:44:08 +00:00)\n"]}],"source":["# Set random seed\n","\n","random.seed(42)\n","np.random.seed(42)\n","tf.random.set_seed(42)"]},{"cell_type":"code","source":["# Disable warning\n","\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","tf.get_logger().setLevel('ERROR')"],"metadata":{"id":"Ktkk46tecq7h","executionInfo":{"status":"ok","timestamp":1711968259464,"user_tz":-60,"elapsed":3,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"77a5fd40-e06c-49ac-b3c5-4cdea55dd6f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 359 µs (started: 2024-04-01 10:44:08 +00:00)\n"]}]},{"cell_type":"code","source":["# Define path for saved models, and video for evalution\n","\n","VIDEO_PATH = f\"{GOOGLE_DRIVE_PATH}/Video\"\n","MODEL_PATH = f\"{GOOGLE_DRIVE_PATH}/Models\""],"metadata":{"id":"5p8eZ0zkW-qp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Tools"],"metadata":{"id":"427Zc14icdGd"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1711712236140,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"jXSEhZYjzYPX","outputId":"9f07e143-5025-48f8-e6f5-68de49373077"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 530 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _get_timestamp(format=\"%Y%m%d-%H%M%S\"):\n","    \"\"\"\n","      Get current timestamp\n","    \"\"\"\n","\n","    current_date = datetime.now()\n","    timestamp = current_date.strftime(format)\n","\n","    return timestamp"]},{"cell_type":"code","source":["def _get_data(path, split_size=None):\n","    \"\"\"\n","      Import data with data splitting for validation set\n","    \"\"\"\n","\n","    # Import data\n","    X, y = _import_data(path)\n","\n","    # If split size defined\n","    if split_size:\n","        X_a, X_b, y_a, y_b = train_test_split(X, y,\n","                                              test_size=split_size,\n","                                              random_state=42,\n","                                              shuffle=True,\n","                                              stratify=y)\n","\n","        return X_a, X_b, y_a, y_b\n","\n","    return X, y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Va7-ATNsnuR","executionInfo":{"status":"ok","timestamp":1711715647064,"user_tz":0,"elapsed":3,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"}},"outputId":"80dfa8a6-38de-4b1e-b3f8-f31b4e2ce17a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 708 µs (started: 2024-03-29 12:34:04 +00:00)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1711712236140,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"CPwEpc7-b9j9","outputId":"f38d217e-ded4-4e1c-c5b2-f5b7961d25c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 752 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _import_data(path, n=None, filenames=None):\n","    \"\"\"\n","      Import images and labels\n","    \"\"\"\n","\n","    images = []\n","    labels = []\n","\n","    # If files not exists, unzip input data\n","    if not Path(path).exists():\n","        _unzip_data()\n","\n","    if not filenames:\n","        # Get index of images and labels\n","        image_path = sorted(os.listdir(f'{path}/images'))\n","        label_path = sorted(os.listdir(f'{path}/labels'))\n","\n","        # Zip to retain index\n","        zipped_lists = list(zip(image_path, label_path))\n","\n","        # Random shuffle to get randomly picture for evaluation\n","        random.shuffle(zipped_lists)\n","\n","        image_path, label_path = zip(*zipped_lists)\n","\n","    else:\n","        image_path = [f'image_{fn}.jpeg' for fn in filenames]\n","        label_path = [f'image_{fn}.txt' for fn in filenames]\n","\n","    i = 0\n","    for img, label in zip(image_path, label_path):\n","        # Get only n pictures\n","        if n is not None:\n","            if i >= n:\n","                break\n","\n","        # Get pixel of images, and read labels inside text files\n","        img_val = io.imread(os.path.join(f'{path}/images', img))\n","        label_val = open(os.path.join(f'{path}/labels', label)).read()\n","\n","        images.append(img_val)\n","        labels.append(int(label_val))\n","\n","        i+=1\n","\n","    return images, labels"]},{"cell_type":"code","source":["def _unzip_data(filename='CV2024_CW_Dataset.zip'):\n","    \"\"\"\n","      Unzip input data\n","      Code obtained from Lab tutorial 06\n","    \"\"\"\n","\n","    zip_path = os.path.join(GOOGLE_DRIVE_PATH, f'CW_Dataset/{filename}')\n","\n","    !cp '{zip_path}' .\n","\n","    !yes|unzip -q '{filename}'\n","\n","    !rm '{filename}'"],"metadata":{"id":"Hss820EatDpZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1711712236140,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"Hwpg9CfVcCeJ","outputId":"bbf185e9-1bc8-48d5-d487-f26894347517"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 462 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _resize(images, size):\n","    \"\"\"\n","      Resize images to desire size\n","    \"\"\"\n","\n","    return [cv2.resize(image, size, interpolation=cv2.INTER_LINEAR) for image in images]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1711712236140,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"PK0uLAszsLDb","outputId":"6f728572-fedc-4a40-f8ab-1a622771f75d"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 400 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _convert_to_array(X, y):\n","    \"\"\"\n","      Convert images to array\n","    \"\"\"\n","\n","    X_array = np.array(X)\n","    y_array = np.array(y)\n","\n","    return X_array, y_array"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1711712236140,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"FH3WtIwB1bPn","outputId":"d167c4bf-ec10-421b-b0de-ee62059cff3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 373 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _convert_to_float(images):\n","    \"\"\"\n","      Convert to float datatype\n","    \"\"\"\n","\n","    return img_as_float(images)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1711712236140,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"JMRjKw_m6yir","outputId":"0aae1b8c-b29a-45e1-c9e0-320d800c756b"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 674 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _convert_to_int(images):\n","    \"\"\"\n","      Convert to int datatype\n","    \"\"\"\n","\n","    return img_as_ubyte(images)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1711712236140,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"PLxXPpwecCgX","outputId":"6c88ff80-c272-482f-8bee-4f46a8abb1e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 377 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _bgr2gray(images):\n","    \"\"\"\n","    Convert BGR to Gray scale\n","    \"\"\"\n","\n","    return cv2.cvtColor(images, cv2.COLOR_BGR2GRAY)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711712236140,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"eINpe0mQ3KV4","outputId":"ffeb88b1-3365-4680-8b78-4297ab19a97f"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 464 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _rgb2gray(images):\n","    \"\"\"\n","    Convert RGB to Gray scale\n","    \"\"\"\n","\n","    images_gray = [cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in images]\n","    return np.array(images_gray)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":422,"status":"ok","timestamp":1711712236560,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"v8V7OBEvcCit","outputId":"ffeea235-f48a-4273-8092-6b0a37566771"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 356 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _normalize(images):\n","    \"\"\"\n","      Normalise image to range between 0 and 1\n","    \"\"\"\n","\n","    return images / 255.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1711712236560,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"IQqx8Z1ifKw9","outputId":"d0b968b7-a65b-409b-9324-abcfb6dc1bae"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 551 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _equalizehist(images):\n","    \"\"\"\n","      Histogram Equalization\n","      Code obtained from https://stackoverflow.com/questions/31998428/opencv-python-equalizehist-colored-image\n","    \"\"\"\n","\n","    equalized = []\n","\n","    for img in images:\n","\n","        # Covert BGR to YCrCb\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n","\n","        # Equalize on Y channel\n","        img[:, :, 0] = cv2.equalizeHist(img[:, :, 0])\n","\n","        # Convert back to BGR\n","        img = cv2.cvtColor(img, cv2.COLOR_YCrCb2BGR)\n","\n","        equalized.append(img)\n","\n","    return np.array(equalized)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1711712236561,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"4FY4Jt21i45I","outputId":"a14118b9-81e4-415b-dff3-d27d1049c0ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 785 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _shuffle(X, y):\n","    \"\"\"\n","      Shuffle records\n","    \"\"\"\n","\n","    X_shuffle, y_shuffle = shuffle(X, y)\n","\n","    return X_shuffle, y_shuffle"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1711712236561,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"NtrWqC8qzcE5","outputId":"b9b4d63b-b624-4fef-e787-2da75ed6a9c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 572 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _randomoversampler(X, y, input_size=(64,64)):\n","    \"\"\"\n","      Oversampling minority class with bootstaping\n","    \"\"\"\n","\n","    # Upsample data with bootstraping\n","    ros = RandomOverSampler(random_state=42)\n","\n","    X_flat = X.reshape(X.shape[0],-1)\n","    X_upsampled, y_upsampled = ros.fit_resample(X_flat, y)\n","    X_upsampled = X_upsampled.reshape(-1, input_size[0], input_size[1], 3)\n","\n","    return X_upsampled, y_upsampled"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1711712236561,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"M1oAO_IQGedW","outputId":"ec14d564-ac4a-4e2e-bf0a-64a6134e0e59"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 451 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _augmentation(rotation_range=10,\n","                  horizontal_flip=True,\n","                  fill_mode='nearest',\n","                  zoom_range=0.1):\n","    \"\"\"\n","      Data augmentation\n","    \"\"\"\n","\n","    # Define an image data generator with specified augmentation parameters\n","    generator = ImageDataGenerator(rotation_range=rotation_range,\n","                                   horizontal_flip=horizontal_flip,\n","                                   fill_mode=fill_mode,\n","                                   zoom_range=zoom_range)\n","\n","    return generator"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1711712236561,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"11bpVrhecCrz","outputId":"bda6a66e-e0fd-4470-aaad-ba0c91b25731"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 637 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _gen_augmented_data(X, y, batch_size=32):\n","    \"\"\"\n","      Data augmentation\n","    \"\"\"\n","\n","    augmented_data = []\n","    augmented_labels = []\n","\n","    # Create a generator for augmented data\n","    generator = _augmentation().flow(_convert_to_float(X), y, batch_size=batch_size)\n","\n","    # Iterate through the generator and collect augmented data until the length of augmented data matches the original data\n","    for X_batch, y_batch in generator:\n","        augmented_data.extend(X_batch)\n","        augmented_labels.extend(y_batch)\n","\n","        if len(augmented_data) >= len(X):\n","            break\n","\n","    # Convert lists to arrays\n","    X_augmented = np.array(_convert_to_int(augmented_data))\n","    y_augmented = np.array(augmented_labels)\n","\n","    return X_augmented, y_augmented"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1711712236561,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"ArolRFkI1knX","outputId":"561b3265-24cb-4b70-c49d-2e53bd916e6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 624 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _gridsearchcv(classifier, X, y, params, folds=5):\n","    \"\"\"\n","    Perform Gridsearch for hyperparameter tuning\n","    \"\"\"\n","\n","    # Use Stratify n-folds cross validation\n","    cv = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n","\n","    # Initialize GridSearchCV object\n","    grid = GridSearchCV(classifier, params, verbose=1, refit=True, n_jobs=-1, scoring=\"f1_macro\", cv=cv)\n","\n","    # Perform grid search\n","    grid.fit(X, y)\n","\n","    print(f\"Best params : {grid.best_params_}\")\n","    print(f\"Best scores : {grid.best_score_}\")\n","\n","    return grid"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1711712236561,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"},"user_tz":0},"id":"oaZok34m0FJb","outputId":"2e9f4356-cf58-4af4-e4fa-e5d9e365c778"},"outputs":[{"output_type":"stream","name":"stdout","text":["time: 423 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}],"source":["def _gen_performance_metrics(y, y_pred):\n","    \"\"\"\n","    Get accuracy and classification report\n","    \"\"\"\n","\n","    # Print classification report for evaluation\n","    accuracy = accuracy_score(y, y_pred)\n","    print(\"Accuracy:\", accuracy)\n","    print(classification_report(y, y_pred))"]},{"cell_type":"code","source":["def _confusion_matrix(y, y_pred):\n","    \"\"\"\n","      Visualize confusion matrix\n","    \"\"\"\n","\n","    # Calculate and visualize confusion matric for evaluation\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(confusion_matrix(y, y_pred), annot=True, fmt=\"d\", cmap=\"Blues\")\n","    plt.xlabel(\"Predicted labels\")\n","    plt.ylabel(\"True labels\")\n","    plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iS_bORHqozev","executionInfo":{"status":"ok","timestamp":1711712236561,"user_tz":0,"elapsed":3,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"}},"outputId":"95e64336-5245-47ae-a373-1b0c3d31c773"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 482 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}]},{"cell_type":"code","source":["def _history_plot(history):\n","    \"\"\"\n","      Plot training history (accuracy and loss)\n","    \"\"\"\n","\n","    # Create subplots\n","    fig, axs = plt.subplots(2, 1, figsize=(10, 8))\n","\n","    history_log_df = pd.DataFrame(history)\n","\n","    # Plot accuracy\n","    sns.lineplot(data=history_log_df[['accuracy','val_accuracy']], ax=axs[0])\n","    axs[0].set_title('Accuracy')\n","\n","    # Plot loss\n","    sns.lineplot(data=history_log_df[['loss','val_loss']], ax=axs[1])\n","    axs[1].set_title('Loss')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-_7hgz6aIPj0","executionInfo":{"status":"ok","timestamp":1711712236561,"user_tz":0,"elapsed":4,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"}},"outputId":"6736f720-ad0e-44b6-950a-f6db9f72bedc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 596 µs (started: 2024-03-29 11:37:13 +00:00)\n"]}]},{"cell_type":"code","source":["def _evaluate(model, history, X_train, y_train, X_validate, y_validate):\n","    \"\"\"\n","      Evaluation on train and validation set\n","    \"\"\"\n","\n","    # Evaluate train set\n","    y_pred_train = model.predict(X_train)\n","    _gen_performance_metrics(np.argmax(y_train, axis=1), np.argmax(y_pred_train, axis=1))\n","\n","    # Evaluate validation set\n","    y_pred_validate = model.predict(X_validate)\n","    _gen_performance_metrics(np.argmax(y_validate, axis=1), np.argmax(y_pred_validate, axis=1))\n","\n","    # Visualize history loss and accuracy\n","    _history_plot(history)"],"metadata":{"id":"eZZvD-7eXUN5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _plot_images_with_labels(X_test, y_test, y_pred_test, label_true, label_pred):\n","    \"\"\"\n","      Plot 4 images with specify true label and predicted labels, to perform error analysis\n","    \"\"\"\n","\n","    count = 0\n","    plt.figure(figsize=(12, 8))\n","    for i in range(len(y_test)):\n","        if y_test[i] == label_true and y_pred_test[i] == label_pred:\n","            plt.subplot(1, 4, count + 1)\n","            plt.imshow(X_test[i])\n","            plt.title(f'True: {y_test[i]}, Predicted: {y_pred_test[i]}')\n","            plt.axis('off')\n","            count += 1\n","            if count == 4:\n","                break\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"VfmN21pdX_HG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _plot_images(X, y_true, y_pred):\n","    \"\"\"\n","      Plot images with true and predicted labels\n","    \"\"\"\n","\n","    num_images = len(y_true)\n","    plt.figure(figsize=(12, 8))\n","    for i in range(num_images):\n","        plt.subplot(1, num_images, i+1)\n","        plt.imshow(X[i])\n","        plt.title(f'True: {y_true[i]}, Predicted: {y_pred[i]}')\n","        plt.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"zeZ0pCxQH-Vr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _video_detection(path, detector, preprocess, model):\n","    \"\"\"\n","      Detect objects in a video using a given detector model\n","      Code derived from Lab 09\n","    \"\"\"\n","\n","    # Load the video file\n","    video_path = f\"{VIDEO_PATH}/{path}\"\n","    cap = cv2.VideoCapture(video_path)\n","    frameCount = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","    frameWidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    frameHeight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","    # Create an array to store processed frames\n","    video = np.empty((frameCount//10, frameHeight, frameWidth, 3), np.dtype('uint8'))\n","    print('video shape =', video.shape)\n","    print(f'Frame count : {frameCount}')\n","\n","    fc = 0\n","    ret = True\n","    i = 0\n","\n","    # Process each frame of the video\n","    while cap.isOpened() and ret and i < frameCount//10:\n","        ret, frame = cap.read()\n","\n","        # Process every 10th frame\n","        if fc % 10 == 0:\n","\n","            # Perform face, and face mask detection\n","            frame = detector(frame, preprocess, model)\n","\n","            # Convert back to RGB\n","            video[i] = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            i += 1\n","\n","        fc += 1\n","\n","    cap.release()\n","\n","    return video, i"],"metadata":{"id":"56M5o1-fqDLU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def face_detector(path, detector, preprocess, model):\n","    \"\"\"\n","      Detect faces in frames' video using a given face detector model\n","      Code derived from Lab 09\n","    \"\"\"\n","\n","    # Process video frames to detect faces\n","    video, frames = _video_detection(path, detector, preprocess, model)\n","    fig, ax = plt.subplots(figsize=(5, 3))\n","\n","    # Create animation of detected faces\n","    def frame(i):\n","        ax.clear()\n","        ax.axis('off')\n","        fig.tight_layout()\n","        plot=ax.imshow(video[i, :, :, :])\n","        return plot\n","\n","    anim = animation.FuncAnimation(fig, frame, frames=frames)\n","    plt.close()\n","\n","    return anim"],"metadata":{"id":"ez3qETGWs6aT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def yunet_face_detector(frame, preprocess, model, lw=10):\n","    \"\"\"\n","      Detect faces in a video frame using Yunet face detection model\n","      Code derived from Lab 08, https://docs.opencv.org/4.x/d0/dd4/tutorial_dnn_face.html\n","      Model obtained from https://github.com/opencv/opencv_zoo/tree/main/models/face_detection_yunet\n","    \"\"\"\n","\n","    # Create Yunet face detector\n","    yn = cv2.FaceDetectorYN.create(f'{GOOGLE_DRIVE_PATH}/Code/face_detection_yunet_2023mar.onnx',  '', (0, 0))\n","    h, w, _ = frame.shape\n","    yn.setInputSize((w, h))\n","\n","    # Detect faces\n","    _, faces = yn.detect(frame)\n","\n","    if faces is not None:\n","        for face in faces:\n","\n","            # Extract detected face\n","            detected_face = frame[int(face[1]):int(face[1])+int(face[3]), int(face[0]):int(face[0])+int(face[2])]\n","\n","            # Predict label for detected face\n","            color, text = _predict_detected_face(detected_face, preprocess, model)\n","\n","            # Draw rectangle around detected face\n","            cv2.rectangle(frame, [int(face[0]), int(face[1]), int(face[2]), int(face[3])], color, lw)\n","\n","            # Add label to the detected face\n","            cv2.putText(frame, text, (int(face[0]), int(face[1])-10), cv2.FONT_HERSHEY_TRIPLEX, 1.5, color, 3)\n","\n","    return frame"],"metadata":{"id":"wq4TbhZhs4RM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712228739013,"user_tz":-60,"elapsed":5,"user":{"displayName":"Kornkamol Sampaongern","userId":"05021282894941745535"}},"outputId":"a8ca0a91-e85d-48bb-da3f-c39066c454a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.81 ms (started: 2024-04-04 11:05:34 +00:00)\n"]}]},{"cell_type":"code","source":["def _predict_detected_face(detected_face, preprocess, model):\n","      \"\"\"\n","        Predict label and confidence score for a detected face\n","      \"\"\"\n","\n","      # Predict labels of face mask classes with confidence score\n","      _, pred, confidence_scores = preprocess._evaluate_frame([detected_face], None, model)\n","      score = f\"{round(confidence_scores[0]*100,1)} %\"\n","\n","      # Define color and text for each class\n","      if pred[0] == 0:\n","        color = (0, 0, 255)\n","        text = f'No Mask, {score}'\n","\n","      elif pred[0] == 1:\n","        color = (0, 255, 0)\n","        text = f'Mask, {score}'\n","\n","      else:\n","        color = (255, 255, 0)\n","        text = f'Mask Incorrect, {score}'\n","\n","      return color, text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OObqjm2d5u8_","executionInfo":{"status":"ok","timestamp":1712228739013,"user_tz":-60,"elapsed":5,"user":{"displayName":"Kornkamol Sampaongern","userId":"05021282894941745535"}},"outputId":"bb717c53-b230-4247-dc7d-656cf8b21479"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.47 ms (started: 2024-04-04 11:05:34 +00:00)\n"]}]},{"cell_type":"code","source":["def _get_model_size(path):\n","    \"\"\"\n","      Get the size of the model file\n","    \"\"\"\n","\n","    # Read and calculate model size\n","    model_path = f\"{MODEL_PATH}/{path}\"\n","\n","    if os.path.isdir(model_path):\n","        model_path = f'{model_path}/model.pkl'\n","\n","    print(f\"Model Size: {os.path.getsize(model_path)} byte\")"],"metadata":{"id":"S6coG3rLOACk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _esrgan(imgs):\n","    \"\"\"\n","      Enhances the input low-resolution image using ESRGAN (Enhanced Super-Resolution Generative Adversarial Network)\n","\n","      Code derived from Tensorflow Hub tutorial,\n","      https://www.tensorflow.org/hub/tutorials/image_enhancing, demonstrating ESRGAN obtained by by Xintao Wang et.al.\n","    \"\"\"\n","\n","    # Load the ESRGAN model from TensorFlow Hub\n","    esrgan_path = \"https://tfhub.dev/captain-pool/esrgan-tf2/1\"\n","    esrgan = hub.load(esrgan_path)\n","\n","    hr_imgs = []\n","\n","    for img in imgs:\n","\n","        # Convert the input image to float32\n","        img = tf.cast(img, tf.float32)\n","\n","        # Crop the input image to a size that is a multiple of 4\n","        target_height = (img.shape[0] // 4) * 4\n","        target_width = (img.shape[1] // 4) * 4\n","        lr = tf.image.crop_to_bounding_box(img, 0, 0, target_height, target_width)\n","\n","        # Convert the cropped image to a TensorFlow tensor\n","        lr = tf.convert_to_tensor(lr, dtype=tf.float32)\n","        lr = tf.expand_dims(lr, 0)\n","\n","        # Apply ESRGAN to the low-resolution image to obtain the high-resolution image\n","        hr = esrgan(lr)\n","        hr_img = tf.squeeze(hr)\n","\n","        # Normalize the pixel values of the high-resolution image to the range [0, 255]\n","        hr_img = (hr_img - np.min(hr_img)) / (np.max(hr_img) - np.min(hr_img)) * 255\n","        hr_img = np.clip(hr_img, 0, 255).astype(np.uint8)\n","\n","        hr_imgs.append(hr_img)\n","\n","    return hr_imgs"],"metadata":{"id":"tNuSShWHJZ_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _gradcam_heatmap(img, model, last_conv):\n","    \"\"\"\n","    Compute the Grad-CAM heatmap for a given input image array\n","    Code obtained from Keras example by fchollet\n","        https://keras.io/examples/vision/grad_cam/\n","    \"\"\"\n","\n","    # Create a model to get the activations of the last conv layer and predictions\n","    model = Model(model.inputs, [model.get_layer(last_conv).output, model.output])\n","\n","    # Compute the gradient of the top predicted class for the input image\n","    with tf.GradientTape() as tape:\n","        output, preds = model(img)\n","        pred_label = tf.argmax(preds[0])\n","        class_channel = preds[:, pred_label]\n","\n","    # Compute the gradient of the output neuron with respect to the last conv layer\n","    grads = tape.gradient(class_channel, output)\n","\n","    # Compute mean intensity of the gradient over each feature map channel\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    # Multiply each channel by importance w.r.t. predicted class and sum for heatmap\n","    heatmap = output[0] @ pooled_grads[..., tf.newaxis]\n","    heatmap = tf.squeeze(heatmap)\n","\n","    # Normalize heatmap for visualization\n","    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","\n","    return heatmap.numpy(), pred_label.numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mUWqpk3v27Ha","executionInfo":{"status":"ok","timestamp":1713530067946,"user_tz":-60,"elapsed":1,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"}},"outputId":"78a95399-1efa-45cb-99dc-371d91a58f4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.56 ms (started: 2024-04-19 12:34:27 +00:00)\n"]}]},{"cell_type":"code","source":["def _gradcam(X, y, input_size, preprocess, last_conv, model_path):\n","    \"\"\"\n","      Generate Grad-CAM visualization from trained model for a given input image\n","      Code obtained from Keras example by fchollet\n","        https://keras.io/examples/vision/grad_cam/\n","    \"\"\"\n","\n","    # Prepare image\n","    img = (_resize(X, input_size))[0]\n","    img = np.expand_dims(img, axis=0)\n","\n","    if preprocess:\n","      img = preprocess(img)\n","\n","    # Load trained model\n","    model = load_model(f\"{MODEL_PATH}/{model_path}\")\n","\n","    # Remove last layer's activation\n","    model.layers[-1].activation = None\n","\n","    # Generate class activation heatmap using Grad-CAM technique\n","    heatmap, label = _gradcam_heatmap(img, model, last_conv)\n","\n","    # Convert the original image to a  array\n","    img = keras.utils.img_to_array(X[0])\n","\n","    # Rescale the heatmap to a range of 0-255\n","    heatmap = np.uint8(255 * heatmap)\n","\n","    # Use jet colormap to colorize heatmap\n","    jet = matplotlib.colormaps[\"jet\"]\n","\n","    # Extract the RGB values from the jet colormap\n","    colors = (jet(np.arange(256))[:, :3])[heatmap]\n","\n","    # Convert the colorized heatmap to an image\n","    colorized_heatmap = keras.utils.array_to_img(colors)\n","    colorized_heatmap = colorized_heatmap.resize((img.shape[1], img.shape[0]))\n","    colorized_heatmap = keras.utils.img_to_array(colorized_heatmap)\n","\n","    # Superimpose the heatmap on the original image\n","    img = colorized_heatmap * 0.5 + img\n","    img = keras.utils.array_to_img(img)\n","\n","    return img, label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KK0M1ift27J5","executionInfo":{"status":"ok","timestamp":1713530068609,"user_tz":-60,"elapsed":2,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"}},"outputId":"cd70b65a-08a3-496e-8197-d6b16879c8a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2.48 ms (started: 2024-04-19 12:34:27 +00:00)\n"]}]},{"cell_type":"code","source":["def _plot_gradcam(imgs, labels, input_size=(224,224), preprocess=None, last_conv=None, model_path=None):\n","    \"\"\"\n","      Plot images along with corresponding Grad-CAM heatmaps\n","    \"\"\"\n","\n","    fig, axes = plt.subplots(1, 3, figsize=(8, 12))\n","\n","    for i, (X, y) in enumerate(zip(imgs, labels)):\n","        img, y_pred = _gradcam([X], y, input_size, preprocess, last_conv, model_path)\n","\n","\n","        # Plot image\n","        ax = axes[i]\n","        ax.imshow(img)\n","        ax.set_title(f\"True: {labels[i]}, Predicted: {y_pred}\")\n","        ax.axis('off')\n","\n","    # Show the plot\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"xoBrhZ2OiRB_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Main Functions"],"metadata":{"id":"-eUHSS9oFAno"}},{"cell_type":"code","source":["def MaskDetection(path_to_testset, model_type, n=4, model_name=None):\n","    \"\"\"\n","      Perform mask detection using the specified model and preprocessing methods\n","\n","      Params:\n","        - path_to_testset: Dataset path to be evaluated\n","        - model_type: Type of model used for detection\n","        - n: Number of test samples to evaluate or 'All' for all samples\n","        - model_name: Specific model name to be evaluated, otherwise default models for each algorithm will be loaded\n","\n","      Returns:\n","        - None\n","    \"\"\"\n","\n","\n","    # Call different functions, and load models\n","    if model_type == 'ResNet50':\n","          model_path = f\"{MODEL_PATH}/Pretrained-CNN/{'20240410-173718.h5' if not model_name else model_name}\"\n","          func = pretrainedCNN(preprocess=resnet50_preprocess)\n","          model = load_model(model_path)\n","\n","    elif model_type == 'VGG16':\n","          model_path = f\"{MODEL_PATH}/Pretrained-CNN/{'20240405-163933.h5' if not model_name else model_name}\"\n","          func = pretrainedCNN(preprocess=vgg16_preprocess)\n","          model = load_model(model_path)\n","\n","    elif model_type == 'MobileNet':\n","          model_path = f\"{MODEL_PATH}/Pretrained-CNN/{'20240405-093232.h5' if not model_name else model_name}\"\n","          func = pretrainedCNN(preprocess=mobilenet_preprocess)\n","          model = load_model(model_path)\n","\n","    elif model_type == 'EfficientNet':\n","          model_path = f\"{MODEL_PATH}/Pretrained-CNN/{'20240410-170150.h5' if not model_name else model_name}\"\n","          func = pretrainedCNN(preprocess=efficientnet_preproces)\n","          model = load_model(model_path)\n","\n","    elif model_type == 'CustomCNN':\n","          model_path = f\"{MODEL_PATH}/Custom-CNN/{'20240409-183732.h5' if not model_name else model_name}\"\n","          func = customCNN()\n","          model = load_model(model_path)\n","\n","    elif model_type == 'SVM-HOG':\n","          model_path = f\"{MODEL_PATH}/SVM-HOG/{'20240404-233425.pkl' if not model_name else model_name}\"\n","          func = SVMWithHOG()\n","          model = joblib.load(model_path)\n","\n","    elif model_type == 'SVM-SIFT':\n","          model_fn = f\"SVM-SIFT/{'20240404-231342' if not model_name else model_name}\"\n","          model_path = f\"{MODEL_PATH}/{model_fn}\"\n","          func = SVMWithSIFT(filepath=model_fn)\n","          # For SVM with SIFT, model is stored inside directory with k-means model\n","          model = joblib.load(f'{model_path}/model.pkl')\n","\n","    # Evaluate entire test set\n","    if n == 'All':\n","      func._evaluate_test_set(model)\n","\n","    # Otherwise, evaluate only n images (default is 4)\n","    else:\n","      # Import test data\n","      X, y = _import_data(path_to_testset, n=n)\n","\n","      # Get predicted labels\n","      y, y_pred, _ = func._evaluate_frame(X, y, model)\n","      _plot_images(X, y, y_pred)"],"metadata":{"id":"E7oUMlhTxNoL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def MaskDetectionVideo(model_path='Pretrained-CNN/20240410-173718.h5', model_type='ResNet50'):\n","    \"\"\"\n","      Detects masks in a video using the specified model\n","\n","      Params:\n","        - model_path: The path to the model, default: 'ResNet50'\n","        - model_type: The type of model to be used, default: 'ResNet50'\n","\n","      Returns:\n","        - Animation: Animation object showing the processed video with mask detection\n","    \"\"\"\n","\n","    # Load model\n","    model = load_model(f\"{MODEL_PATH}/{model_path}\")\n","    print('Model Loaded successcfully!')\n","\n","    # Call different functions depends on models\n","    if model_type == 'ResNet50':\n","          func = pretrainedCNN(preprocess=resnet50_preprocess)\n","\n","    elif model_type == 'VGG16':\n","          func = pretrainedCNN(preprocess=vgg16_preprocess)\n","\n","    elif model_type == 'MobileNet':\n","          func = pretrainedCNN(preprocess=mobilenet_preprocess)\n","\n","    elif model_type == 'EfficientNet':\n","          func = pretrainedCNN(preprocess=efficientnet_preproces)\n","\n","    elif model_type == 'CustomCNN':\n","          func = customCNN()\n","\n","    elif model_type == 'SVM-HOG':\n","          func = SVMWithHOG()\n","\n","    elif model_type == 'SVM-SIFT':\n","          func = SVMWithSIFT()\n","\n","    # Perform face mask detector using specified model\n","    anim = face_detector('Video1.mp4', yunet_face_detector, func, model)\n","    print('Precessing video complete!')\n","\n","    return anim"],"metadata":{"id":"hLdZvw-NVW5Y"},"execution_count":null,"outputs":[]}]}