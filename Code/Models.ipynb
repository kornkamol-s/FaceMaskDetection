{"cells":[{"cell_type":"markdown","metadata":{"id":"cK2Gt8SKDlem"},"source":["### **SVM**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9TRtL2w00KLU"},"outputs":[],"source":["class SVMInitiate():\n","    \"\"\"\n","      Encapsulate entire process of SVM pipeline, including data preprocessing,\n","      feature extraction, hyper parameter tuing, training, and evaluation\n","\n","    \"\"\"\n","\n","    def __init__(self, params={}, input_size=(64,64), max_iter=-1, augmentation=False, balancing=False, filepath=None):\n","        \"\"\"\n","        Params:\n","          - params: Dictionary containing parameters to be optimized using grid search\n","          - input_size: Tuple indicating the desired input image size for resizing\n","          - max_iter: Maximum number of iterations for SVM training\n","          - augmentation: Boolean indicating whether to apply data augmentation\n","          - balancing: Boolean indicating whether to apply class balancing\n","          - filepath: String indicating the model filename to be saved\n","        \"\"\"\n","\n","        self.params = params\n","        self.input_size = input_size\n","        self.augmentation = augmentation\n","        self.balancing = balancing\n","        self.max_iter = max_iter\n","        self.filepath = filepath\n","\n","\n","    def _get_data(self, path):\n","        \"\"\"\n","        Import data\n","\n","        Params:\n","          - path: String indicating the path from which to import the data (train, test)\n","\n","        Returns:\n","          - List containing the values, and labels\n","        \"\"\"\n","\n","        return _import_data(path)\n","\n","\n","    def _preprocess(self, X, y, equalizehist=False, augmentation=False, balancing=False):\n","        \"\"\"\n","        Preprocess input data\n","\n","        Params:\n","          - X: Input data\n","          - y: Labels corresponding to input data\n","          - equalizehist: Boolean indicating whether to apply histogram equalization\n","          - augmentation: Boolean indicating whether to apply data augmentation\n","          - balancing: Boolean indicating whether to apply class balancing\n","\n","        Returns:\n","          - Array of preprocessed input data and labels\n","        \"\"\"\n","\n","        # Resize input data\n","        X = _resize(X, self.input_size)\n","\n","        # Convert to array\n","        X, y = _convert_to_array(X, y)\n","\n","        # Apply equilized histogram, if specify\n","        if equalizehist:\n","            X = _equalizehist(X)\n","\n","        # Apply balaning, if specify\n","        if balancing:\n","\n","            # Downsampling majority class 1\n","            X_train_filter = X[y == 1]\n","            y_data_filter = y[y == 1]\n","\n","            X_train_filter, y_data_filter = resample(X_train_filter, y_data_filter, replace=False, n_samples=500, random_state=42)\n","\n","            # Upsampling all the other classes\n","            X_train_other = X[y != 1]\n","            y_train_other = y[y != 1]\n","\n","            X_train_combined = np.concatenate([X_train_filter, X_train_other])\n","            y_train_combined = np.concatenate([y_data_filter, y_train_other])\n","\n","            X, y = self.balancing(X_train_combined, y_train_combined)\n","\n","            # Shuffle input data and label, preventing bias\n","            X, y = _shuffle(X, y)\n","\n","        # Apply augmentation, if specify\n","        if augmentation:\n","              X, y = _gen_augmented_data(X, y)\n","              X, y = _shuffle(X, y)\n","\n","        return X, y\n","\n","\n","    def _get_features(self):\n","        \"\"\"\n","        Extract features\n","\n","        Returns:\n","          - None\n","        \"\"\"\n","\n","        pass\n","\n","    def _main(self):\n","        \"\"\"\n","        Encapsulate the training process, from importing data to evaluation on train and validation set\n","\n","        Returns:\n","          - None\n","        \"\"\"\n","\n","        # Use current timestamp as model filename\n","        model_fn = _get_timestamp()\n","\n","        # Import input data\n","        self.X_train, self.X_validate, self.y_train, self.y_validate = _get_data('train', split_size=0.2)\n","\n","        # Preprocess train data\n","        self.X_train, self.y_train = self._preprocess(self.X_train, self.y_train, equalizehist=True, augmentation=self.augmentation, balancing=self.balancing)\n","\n","        # Feature extraction using either SIFT or HOG\n","        self.X_train, self.y_train = self._get_features(self.X_train, self.y_train, types='train', fn=model_fn)\n","\n","        self.filepath = model_fn\n","\n","        # Preprocess validation data\n","        self.X_validate, self.y_validate = self._preprocess(self.X_validate, self.y_validate)\n","\n","        # Feature extraction using either SIFT or HOG\n","        self.X_validate, self.y_validate = self._get_features(self.X_validate, self.y_validate, types='test')\n","\n","        # Initilize SVM model\n","        classifier = svm.SVC(class_weight='balanced', random_state=42, tol=1e-3, max_iter=self.max_iter)\n","\n","        # Perform parameter tuning using grid search cross validation\n","        grid_search = _gridsearchcv(classifier, self.X_train, self.y_train, self.params)\n","\n","        # Get best model\n","        best_classifier = grid_search.best_estimator_\n","\n","        # Save best model to drive\n","        print(f\"Model saved to {model_fn}\")\n","        self._save_model(best_classifier, model_fn)\n","\n","        # Evaluate on train set\n","        y_pred = best_classifier.predict(self.X_train)\n","        _gen_performance_metrics(self.y_train, y_pred)\n","\n","        # Evaluate on validation set\n","        y_pred = best_classifier.predict(self.X_validate)\n","        _gen_performance_metrics(self.y_validate, y_pred)\n","\n","\n","    def _evaluate_frame(self, X, y, model):\n","        \"\"\"\n","        Evaluate subset of test images, including preprocessing, feature extraction, and model prediction\n","\n","        Params:\n","          - X: Input data\n","          - y: True label corresponding to the input data\n","          - model: Trained model for prediction\n","\n","        Returns:\n","          - True labels, predicted labels, and any additional evaluation result\n","        \"\"\"\n","\n","        # Preprocess test data\n","        X, y = self._preprocess(X, y)\n","\n","        # Feature extraction either SIFT or HOG\n","        X, y = self._get_features(X, y, types='test')\n","\n","        # Get predicted results\n","        y_pred = model.predict(X)\n","\n","        return y, y_pred, None\n","\n","\n","    def _evaluate_test_set(self, model, label_true=None, label_pred=None):\n","        \"\"\"\n","        Evaluate the entire test data\n","\n","        Params:\n","          - model: Trained model for prediction\n","          - label_true: True label, to visualize image with this true label\n","          - label_pred: Predicted label, to visualize image with this predicted label\n","\n","        Returns:\n","          - None\n","        \"\"\"\n","\n","        # Import test data\n","        X_test, y_test = self._get_data('test')\n","\n","        # Preprocess test data\n","        X_test, y_test = self._preprocess(X_test, y_test)\n","\n","        # Feature extraction using either SIFT or HOG\n","        X_test, y_test = self._get_features(X_test, y_test, types='test')\n","\n","        # Get predicted results\n","        y_pred = model.predict(X_test)\n","\n","        # if label_true not provided, visualize classification report and confusion matrix\n","        if label_true is None:\n","            _gen_performance_metrics(y_test, y_pred)\n","            _confusion_matrix(y_test, y_pred)\n","\n","        # To visualize images with particular true labels, and predicted labels\n","        else:\n","          _plot_images_with_labels(X_test, y_test, y_pred, label_true, label_pred)\n"]},{"cell_type":"markdown","metadata":{"id":"2-dUN_5b1bK5"},"source":["###**HOG**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcBmMXiY3WJM"},"outputs":[],"source":["class SVMWithHOG(SVMInitiate):\n","    \"\"\"\n","    Inherit from SVMInitiate class to replace feature extraction using HOG\n","    Code derived from Lab tutorial 05\n","    \"\"\"\n","\n","    def _get_features(self, X, y, **kwargs):\n","        \"\"\"\n","        Extract features using Histogram of Oriented Gradients (HOG)\n","\n","        Params:\n","          - X: Input images\n","          - y: Labels corresponding to the input images\n","          - **kwargs: Additional keyword arguments\n","\n","        Returns:\n","          - The extracted HOG features and labels\n","        \"\"\"\n","\n","        hog_features = []\n","\n","        # Iterate over each image in the dataset\n","        for i in range(len(X)):\n","\n","            # Compute HOG features\n","            features = hog(X[i],\n","                          orientations=9,\n","                          pixels_per_cell=(8, 8),\n","                          cells_per_block=(2, 2),\n","                          visualize=False,\n","                          channel_axis=2,\n","                          )\n","\n","            # Append computed features to the list\n","            hog_features.append(features)\n","\n","        return np.array(hog_features), y\n","\n","\n","    def _save_model(self, model, fn):\n","        \"\"\"\n","        Save the best model\n","\n","        Params:\n","          - model: Trained SVM model to be saved\n","          - fn: Filename for the saved model\n","\n","        Returns:\n","          - None\n","        \"\"\"\n","\n","        # Create directory\n","        os.makedirs(f'{MODEL_PATH}/SVM-HOG', exist_ok=True)\n","\n","        # Save best model to drive\n","        joblib.dump(model, f'{MODEL_PATH}/SVM-HOG/{fn}.pkl')"]},{"cell_type":"markdown","metadata":{"id":"SKzqsMASDkNH"},"source":["### **SIFT**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zzjaZzc655Zl"},"outputs":[],"source":["class SVMWithSIFT(SVMInitiate):\n","    \"\"\"\n","    Inherit from SVMInitiate class to replace feature extraction using SIFT\n","    Code obtained from Lab tutorial 06\n","    \"\"\"\n","\n","    def _get_features(self, X, y, types=None, fn=None):\n","        \"\"\"\n","        Extract features using Scale-Invariant Feature Transform (SIFT)\n","\n","        Params:\n","          - X: Input images\n","          - y: Labels corresponding to the input images\n","          - types: Type of data (train, test)\n","          - fn: Filename for the saved model\n","\n","        Returns:\n","          - The extracted SIFT-BoVW features and labels\n","        \"\"\"\n","\n","        cluster=30\n","\n","        def _sift_bovw_features(X, y, fn):\n","            \"\"\"\n","            Generate SIFT - Bag of Visual Words (BoVW) representation feature descriptors for training data\n","            \"\"\"\n","\n","            # Apply augmentation, if specify\n","            if self.augmentation:\n","                # Convert to grey scale\n","                X = _rgb2gray(_convert_to_int(X))\n","\n","            else:\n","                # Convert to grey scale\n","                X = _rgb2gray(X)\n","\n","            # Initialise SIFT\n","            sift = cv2.SIFT_create()\n","\n","            des_list = []\n","            y_list = []\n","\n","            for i in range(len(X)):\n","\n","                # Identify keypoints and descriptors\n","                kp, des = sift.detectAndCompute(X[i], None)\n","\n","                # Keep list of descriptors\n","                if des is not None:\n","                    des_list.append(des)\n","                    y_list.append(y[i])\n","\n","            # Convert list to array\n","            descriptors = np.vstack(des_list)\n","\n","            # Generate BoVW representation using MiniBatchKMeans for faster computation and lower memory usage\n","            batch_size = descriptors.shape[0] // 4\n","            kmeans = MiniBatchKMeans(n_clusters=cluster, batch_size=batch_size, n_init='auto').fit(descriptors)\n","\n","            # Convert descriptors into histograms of codewords for each image\n","            hist_list = []\n","            idx_list = []\n","\n","            for des in des_list:\n","                # Initialize a histogram of zeros\n","                hist = np.zeros(cluster)\n","\n","                # Predict cluster indices\n","                idx = kmeans.predict(des)\n","                idx_list.append(idx)\n","\n","                # Update histogram counts based on cluster indices\n","                for j in idx:\n","                    hist[j] = hist[j] + (1 / len(des))\n","                hist_list.append(hist)\n","\n","            # Stack histograms into an array\n","            hist_array = np.vstack(hist_list)\n","\n","            return hist_array, y_list, kmeans\n","\n","\n","        def _sift_bovw_features_test(X, y, kmeans):\n","            \"\"\"\n","            Generate SIFT - Bag of Visual Words (BoVW) representation feature descriptors for test data\n","            \"\"\"\n","\n","            # Convert to grey scale\n","            X = _rgb2gray(X)\n","\n","            hist_list = []\n","\n","            for i in range(len(X)):\n","\n","                # Initialise SIFT\n","                sift = cv2.SIFT_create()\n","\n","                # Identify keypoints and descriptors\n","                kp, des = sift.detectAndCompute(X[i], None)\n","\n","                # Check if descriptors are found\n","                if des is not None:\n","                    # Initialize an array to store the histogram\n","                    hist = np.zeros(cluster)\n","\n","                    # Predict cluster indices for descriptors\n","                    idx = kmeans.predict(des)\n","\n","                    # Update histogram counts based on cluster indices\n","                    for j in idx:\n","                        hist[j] = hist[j] + (1 / len(des))\n","\n","                    hist_list.append(hist)\n","\n","                else:\n","                    hist_list.append(None)\n","\n","\n","            # Remove potential cases of images with no descriptors\n","            idx_not_empty = [i for i, x in enumerate(hist_list) if x is not None]\n","            hist_list = [hist_list[i] for i in idx_not_empty]\n","            y = [y[i] for i in idx_not_empty]\n","\n","            # Stack histograms into an array\n","            hist_array = np.vstack(hist_list)\n","\n","            return hist_array, y\n","\n","        if types == 'train':\n","            # Extract features for training data\n","            X, y, kmeans = _sift_bovw_features(X, y, fn)\n","\n","            # Create directory to save model\n","            os.makedirs(f'{MODEL_PATH}/SVM-SIFT/{fn}/', exist_ok=True)\n","\n","            # Save KMeans model\n","            joblib.dump(kmeans, f'{MODEL_PATH}/SVM-SIFT/{fn}/kmeans.pkl')\n","\n","        elif types == 'test':\n","           if not 'SVM-SIFT' in self.filepath:\n","              self.filepath = f\"SVM-SIFT/{self.filepath}\"\n","\n","           # Load KMeans model\n","           kmeans = joblib.load(f'{MODEL_PATH}/{self.filepath}/kmeans.pkl')\n","\n","           # Extract features for test data\n","           X, y = _sift_bovw_features_test(X, y, kmeans)\n","\n","        return X, y\n","\n","\n","    def _save_model(self, model, fn):\n","        \"\"\"\n","        Save the best model\n","\n","        Params:\n","          - model: Trained SVM model to be saved\n","          - fn: Filename for the saved model\n","\n","        Returns:\n","          - None\n","        \"\"\"\n","\n","        # Save model to drive\n","        joblib.dump(model, f'{MODEL_PATH}/SVM-SIFT/{fn}/model.pkl')"]},{"cell_type":"markdown","metadata":{"id":"r-Ax59YTCeuj"},"source":["### **CNN - Custom**"]},{"cell_type":"code","source":["class customCNN():\n","    \"\"\"\n","      Encapsulate entire process of CNN pipeline, including data preprocessing,\n","      CNN structure, hyper parameter tuing, training, and evaluation\n","    \"\"\"\n","\n","    def __init__(self, params={}, input_size=(64,64), augmentation=False,\n","                 class_weight=False, sr=False,\n","                 epochs=100, batch_size=32):\n","        \"\"\"\n","        Initialize the CustomCNN object\n","\n","        Params:\n","          - params: Dictionary of hyperparameters for the CNN model\n","          - input_size: Tuple representing the desired input image size\n","          - augmentation: Boolean indicating whether data augmentation should be applied\n","          - class_weight: Boolean indicating whether class weights should be used during training\n","          - sr: Boolean indicating whether to apply super resolution\n","          - epochs: Number of maximum epochs\n","          - batch_size: Batch size\n","        \"\"\"\n","\n","        self.params = params\n","        self.input_size = input_size\n","        self.augmentation = augmentation\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.class_weight = class_weight\n","        self.sr = sr\n","\n","\n","    def _preprocess_data(self, X, y, augmentation=False):\n","        \"\"\"\n","        Preprocess input data\n","\n","        Params:\n","          - X: Input images\n","          - y: Labels corresponding to the input images\n","          - augmentation: Boolean indicating whether data augmentation should be applied\n","\n","        Returns:\n","          - Preprocessed input data and labels\n","        \"\"\"\n","\n","        # Resize and convert to array\n","        X = np.array(_resize(X, self.input_size))\n","        y = np.array(y)\n","\n","        # Apply equalized histogram\n","        X = _equalizehist(X)\n","\n","        # Apply augmentation, if specify\n","        if augmentation:\n","              X, y = _gen_augmented_data(X, y)\n","              X, y = _shuffle(X, y)\n","\n","        # Normalize pixel values to the range 0 and 1\n","        X = X / 255.0\n","\n","        # Convert class labels to one-hot encoded vectors\n","        y = to_categorical(y, 3)\n","\n","        return X, y\n","\n","\n","    def _callback(self):\n","        \"\"\"\n","        Define callback function for training\n","\n","        Returns:\n","          - List of callbacks including Model checkpoint, Early stopping, and Learning rate scheduler\n","        \"\"\"\n","\n","        # Model filepath\n","        model_fp = f\"{MODEL_PATH}/Custom-CNN/{_get_timestamp()}.h5\"\n","        print(f\"Model saved to {model_fp}\")\n","\n","        # Define model checkpoint to save the best model based on validation loss\n","        check_point = ModelCheckpoint(filepath=model_fp, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1)\n","\n","        # Define early stopping to stop training if validation loss doesn't improve after certain patience epochs\n","        early_stop = EarlyStopping(monitor='val_loss', patience=15, mode='min', verbose=1)\n","\n","        # Define learning rate scheduler to adjust learning rate if validation loss doesn't improve after 5 epochs\n","        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-5)\n","\n","        return [check_point, early_stop, lr_scheduler]\n","\n","\n","    def _train(self, **kwargs):\n","          \"\"\"\n","          Train the CNN model\n","\n","          Params:\n","            - kwargs: Additional keyword arguments\n","\n","          Returns:\n","            - Training history and trained model\n","          \"\"\"\n","\n","          # Perform randomized search with cross validation\n","          grid = RandomizedSearchCV(estimator=self._create_keras_classifier(callbacks=self._callback(), **kwargs),\n","                                    param_distributions=self.params, n_jobs=1, cv=5, verbose=1, scoring=\"f1_macro\")\n","\n","          if self.class_weight:\n","                # Fit the grid search with class weights\n","                grid_result = grid.fit(self.X_train, self.y_train, class_weight=self.weight)\n","\n","          else:\n","                # Fit the grid search without class weights\n","                grid_result = grid.fit(self.X_train, self.y_train)\n","\n","          # Obtain best model\n","          classifier = grid_result.best_estimator_\n","\n","          print(f\"Best Estimator Parameters: {classifier.get_params()}\")\n","          print(classifier.model_.summary())\n","\n","          if self.class_weight:\n","              # Fit the best estimator with class weights\n","              history = classifier.fit(self.X_train,\n","                                      self.y_train,\n","                                      validation_data =(self.X_validate, self.y_validate),\n","                                      epochs=self.epochs,\n","                                      batch_size=self.batch_size,\n","                                      class_weight=self.weight)\n","\n","          else:\n","              # Fit the best estimator without class weights\n","              history = classifier.fit(self.X_train,\n","                                      self.y_train,\n","                                      validation_data =(self.X_validate, self.y_validate),\n","                                      epochs=self.epochs,\n","                                      batch_size=self.batch_size)\n","\n","          return history, classifier\n","\n","\n","    def _main(self, **kwargs):\n","        \"\"\"\n","        Execute the entire training process, including importing data,\n","        training, and evaluation\n","\n","        Returns:\n","          - None\n","        \"\"\"\n","\n","        # Import data\n","        self.X_train, self.X_validate, self.y_train, self.y_validate = _get_data('train', split_size=0.2)\n","\n","        # Apply super resolution, if specify\n","        if self.sr:\n","            # Unzip super resolution images\n","            if not os.path.exists('esrgan_train_imgs.pkl'):\n","                _unzip_data(filename='esrgan_train_imgs.zip')\n","\n","            # Load super resolution images\n","            with open('esrgan_train_imgs.pkl', 'rb') as f:\n","                data = pickle.load(f)\n","\n","            # Extract X_hr and y_train from the loaded data\n","            self.X_train = data['X']\n","            self.y_train = data['y']\n","\n","        # Calculate class weights using class frequency\n","        self.weight = dict(enumerate(compute_class_weight('balanced', classes=np.unique(self.y_train), y=self.y_train)))\n","\n","        # Preprocess train data\n","        self.X_train, self.y_train = self._preprocess_data(self.X_train, self.y_train, augmentation=self.augmentation)\n","\n","        # Preprocess validation data\n","        self.X_validate, self.y_validate = self._preprocess_data(self.X_validate, self.y_validate)\n","\n","        # Get trained history, and trained model\n","        history, classifier = self._train(**kwargs)\n","\n","        # Evaluate model's performance on train and validation set\n","        _evaluate(classifier, history.history_, self.X_train, self.y_train, self.X_validate.astype('float32'), self.y_validate)\n","\n","\n","    def _evaluate_test_set(self, model, label_true=None, label_pred=None):\n","        \"\"\"\n","        Evaluate the entire test set\n","\n","        Params:\n","          - model: Trained model to be evaluated\n","          - label_true: True label, to visualize image with this true label\n","          - label_pred: Predicted label, to visualize image with this predicted label\n","\n","        Returns:\n","          - None\n","        \"\"\"\n","\n","        # Import and preprocess test set\n","        self.X_test, self.y_test = _get_data('test')\n","        self.X_test, self.y_test = self._preprocess_data(self.X_test, self.y_test)\n","\n","        # Predict test data\n","        y_pred_test = np.argmax(model.predict(self.X_test), axis=1)\n","        self.y_test = np.argmax(self.y_test, axis=1)\n","\n","        # if label_true not provided, visualize classification report and confusion matrix\n","        if label_true is None:\n","            _gen_performance_metrics(self.y_test, y_pred_test)\n","            _confusion_matrix(self.y_test, y_pred_test)\n","\n","        # To visualize images with particular true labels, and predicted labels\n","        else:\n","          _plot_images_with_labels(self.X_test, self.y_test, y_pred_test, label_true, label_pred)\n","\n","\n","    def _evaluate_frame(self, X, y, model):\n","        \"\"\"\n","        Evaluate subset of test images, including preprocessing, feature extraction, and model prediction\n","\n","        Params:\n","          - X: Input test image\n","          - y: True label of the test image\n","          - model: Trained model for prediction\n","\n","        Returns:\n","          - True label, predicted label, and confidence scores\n","        \"\"\"\n","\n","        # Preprocess test image\n","        X, _ = self._preprocess_data(X, [])\n","\n","        # Get predicted label\n","        y_probs = model.predict(X, verbose=0)\n","\n","        # Get the predicted labels with the highest probability\n","        y_pred = np.argmax(y_probs, axis=1)\n","\n","        # Get the confidence scores\n","        confidence_scores = np.max(y_probs, axis=1)\n","\n","        return y, y_pred, confidence_scores\n","\n","\n","    def _create_keras_classifier(self, **kwargs):\n","        \"\"\"\n","        Create a Keras classifier model\n","\n","        Params:\n","          - kwargs: Additional keyword arguments\n","\n","        Returns:\n","          - KerasClassifier object\n","        \"\"\"\n","\n","        def _create_cnn_model(learning_rate, dense_units, dropout_rates, weight_decay):\n","              \"\"\"\n","              Create a CNN model\n","\n","              Params:\n","                - learning_rate: Learning rate for the optimizer\n","                - dense_units: Hidden neuron size\n","                - dropout_rates: Dropout rate for regularization\n","                - weight_decay: Weight decay for regularization\n","\n","              Returns:\n","                - Compiled Keras model\n","              \"\"\"\n","\n","              model = Sequential()\n","\n","              # First convolution layer\n","              model.add(Conv2D(32, (5, 5), activation='relu',kernel_initializer='he_uniform', input_shape=self.input_size + (3,)))\n","\n","              # Max pooling layer\n","              model.add(MaxPooling2D((2, 2)))\n","\n","              # Drop out\n","              model.add(Dropout(dropout_rates))\n","\n","              # Second convolution layer\n","              model.add(Conv2D(32, (5, 5), activation='relu', kernel_initializer='he_uniform'))\n","\n","              # Max pooling layer\n","              model.add(MaxPooling2D((2, 2)))\n","\n","              # Drop out\n","              model.add(Dropout(dropout_rates))\n","\n","              # Flatten layer\n","              model.add(Flatten())\n","\n","              # Fully connected layer\n","              model.add(Dense(dense_units, activation='relu', kernel_initializer='he_uniform'))\n","\n","              # Drop out\n","              model.add(Dropout(dropout_rates))\n","\n","              # Output layer with softmax activation\n","              model.add(Dense(3, activation='softmax'))\n","\n","              # Compile model with the specified optimizer, learning rate, and weight decay\n","              model.compile(Adam(learning_rate=learning_rate, weight_decay=weight_decay), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","              return model\n","\n","        return KerasClassifier(build_fn=_create_cnn_model, **kwargs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3EL1wF7FwMK8","executionInfo":{"status":"ok","timestamp":1711761581336,"user_tz":0,"elapsed":12,"user":{"displayName":"kornkamol Sampaongern","userId":"06824027631227758844"}},"outputId":"7af90c30-bd7c-4812-b846-c7bb78928211"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.97 ms (started: 2024-03-30 01:19:36 +00:00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"yaZQxorPL2g1"},"source":["### **CNN - Transfer learning using pre-trained model**"]},{"cell_type":"code","source":["class pretrainedCNN():\n","    \"\"\"\n","      Encapsulate entire process of CNN pipeline, including data preprocessing,\n","      CNN structure, hyper parameter tuing, training, and evaluation.\n","    \"\"\"\n","\n","    def __init__(self, input_size=(224, 224), augmentation=False, sr=False,\n","                 base_model=MobileNetV2, preprocess=mobilenet_preprocess,\n","                 finetune=False,\n","                 early_stop_patience=10, batch_size=32, epochs=200,\n","                 model_path=None, class_weight=False):\n","        \"\"\"\n","        Initialize the pretrainedCNN object\n","\n","        Params:\n","          - input_size: Tuple representing the desired input image size\n","          - augmentation: Whether to apply data augmentation\n","          - sr: Whether to apply super-resolution\n","          - base_model: Base CNN model to be used\n","          - preprocess: Preprocessing function for input images\n","          - finetune: Whether to fine-tune the base model\n","          - early_stop_patience: Threshold for early stopping during training\n","          - batch_size: Batch size for training\n","          - epochs: Number of epochs for training\n","          - model_path: Path to save the trained model\n","          - class_weight: Whether to apply class weighting during training\n","\n","        \"\"\"\n","\n","        self.input_size = input_size\n","        self.augmentation = augmentation\n","        self.base_model = base_model\n","        self.preprocess = preprocess\n","        self.finetune = finetune\n","        self.early_stop_patience = early_stop_patience\n","        self.batch_size = batch_size\n","        self.epochs = epochs\n","        self.model_path = model_path\n","        self.sr = sr\n","        self.class_weight = class_weight\n","\n","\n","    def _preprocess_data(self, X, y, augmentation=False, **kwargs):\n","        \"\"\"\n","        Preprocess input data\n","\n","        Params:\n","          - X: Input data\n","          - y: Target labels\n","          - augmentation: Whether to apply data augmentation\n","\n","        Returns:\n","          - Preprocessed input data and labels\n","        \"\"\"\n","\n","        # Resize and convert to array\n","        X = np.array(_resize(X, self.input_size))\n","        y = np.array(y)\n","\n","        # Apply augmentation, if specify\n","        if augmentation:\n","            X, y = _gen_augmented_data(X, y)\n","\n","        # Preprocess input\n","        X = self.preprocess(X)\n","\n","        # Convert class labels to one-hot encoded vectors\n","        y = to_categorical(y, 3)\n","\n","        return X, y\n","\n","\n","    def _train(self, **kwargs):\n","        \"\"\"\n","        Training process\n","\n","        Params:\n","          - kwargs: Additional arguments\n","\n","        Returns:\n","            - None\n","        \"\"\"\n","\n","        # Initialize a RandomSearch tuner with the specified parameters\n","        tuner = RandomSearch(partial(_build_model, base_model=self.base_model, input_size=self.input_size, finetune=self.finetune),\n","                             objective='val_loss',\n","                             max_trials=10,\n","                             executions_per_trial=1,\n","                             directory='tmp/',\n","                             project_name='pretrainedCNN')\n","\n","        # Perform hyperparameter search with or without class weights\n","        if self.class_weight:\n","            tuner.search(self.X_train, self.y_train, epochs=5, validation_data=(self.X_validate, self.y_validate), class_weight=self.weight, callbacks=self._callback())\n","\n","        else:\n","            tuner.search(self.X_train, self.y_train, epochs=5, validation_data=(self.X_validate, self.y_validate), callbacks=self._callback())\n","\n","        # Retrieve the best models and hyperparameters\n","        models = tuner.get_best_models(num_models=1)\n","        best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n","        print(f\"Best Hyperparameters: {best_hyperparameters.values}\")\n","\n","        classifier = models[0]\n","\n","\n","        if self.class_weight:\n","            # Fit the best estimator with class weights\n","            history = classifier.fit(self.X_train,\n","                                    self.y_train,\n","                                    validation_data =(self.X_validate, self.y_validate),\n","                                    epochs=self.epochs,\n","                                    batch_size=self.batch_size,\n","                                    class_weight=self.weight,\n","                                    callbacks=self._callback())\n","        else:\n","            # Fit the best estimator without class weights\n","            history = classifier.fit(self.X_train,\n","                                    self.y_train,\n","                                    validation_data =(self.X_validate, self.y_validate),\n","                                    epochs=self.epochs,\n","                                    batch_size=self.batch_size,\n","                                    callbacks=self._callback())\n","\n","        return history, classifier\n","\n","\n","    def _evaluate_test_set(self, model, label_true=None, label_pred=None):\n","        \"\"\"\n","        Evaluate the entire test set\n","\n","        Params:\n","          - model: Trained model\n","          - label_true: True label, to visualize image with this true label\n","          - label_pred: Predicted label, to visualize image with this predicted label\n","\n","        Returns:\n","          - None\n","        \"\"\"\n","\n","        # Import and preprocess test set\n","        X_test, y_test = _get_data('test')\n","        X_test_processed, y_test = self._preprocess_data(X_test, y_test)\n","\n","        # Get predicted labels\n","        y_pred_test = np.argmax(model.predict(X_test_processed), axis=1)\n","\n","        # Convert y test from onehot encoded vector to 1D-array\n","        y_test = np.argmax(y_test, axis=1)\n","\n","        # if label_true not provided, visualize classification report and confusion matrix\n","        if label_true is None:\n","            _gen_performance_metrics(y_test, y_pred_test)\n","            _confusion_matrix(y_test, y_pred_test)\n","\n","        # To visualize images with particular true labels, and predicted labels\n","        else:\n","          _plot_images_with_labels(X_test, y_test, y_pred_test, label_true, label_pred)\n","\n","\n","    def _evaluate_frame(self, X, y, model):\n","        \"\"\"\n","        Evaluate subset of test images, including preprocessing, feature extraction, and model prediction\n","\n","        Params:\n","          - X: Input test image\n","          - y: True label of the test image\n","          - model: Trained model for prediction\n","\n","        Returns:\n","          - True label, predicted label, and confidence scores\n","        \"\"\"\n","\n","        # Preprocess test image\n","        X, _ = self._preprocess_data(X, [])\n","\n","        # Get predicted output\n","        y_probs = model.predict(X, verbose=0)\n","\n","        # Get the predicted labels with the highest probability\n","        y_pred = np.argmax(y_probs, axis=1)\n","\n","        # Get the confidence scores\n","        confidence_scores = np.max(y_probs, axis=1)\n","\n","        return y, y_pred, confidence_scores\n","\n","\n","    def _callback(self):\n","        \"\"\"\n","        Define callback function for training\n","\n","        Returns:\n","          - List of callbacks including Model checkpoint, Early stopping, and Learning rate scheduler\n","        \"\"\"\n","\n","        # Model filepath\n","        model_fp = f\"{MODEL_PATH}/Pretrained-CNN/{_get_timestamp()}.h5\"\n","        print(f\"Model saved to {model_fp}\")\n","\n","        # Define model checkpoint to save the best model based on validation loss\n","        check_point = ModelCheckpoint(filepath=model_fp, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1)\n","\n","        # Define early stopping to stop training if validation loss doesn't improve after certain patience epochs\n","        early_stop = EarlyStopping(monitor='val_loss', patience=self.early_stop_patience, mode='min', verbose=1)\n","\n","        # Define learning rate scheduler to adjust learning rate if validation loss doesn't improve after 5 epochs\n","        lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-5)\n","\n","        return [check_point, early_stop, lr_scheduler]\n","\n","\n","    def _main(self, **kwargs):\n","        \"\"\"\n","        Entire training process, from importing data to evaluation on train and validation set\n","\n","        Params:\n","        - kwargs: Additional arguments\n","        \"\"\"\n","\n","        # Import data\n","        self.X_train, self.X_validate, self.y_train, self.y_validate = _get_data('train', split_size=0.2)\n","\n","        # Apply super resolution, if specify\n","        if self.sr:\n","            # Unzip super resolution images\n","            if not os.path.exists('esrgan_train_imgs.pkl'):\n","                _unzip_data(filename='esrgan_train_imgs.zip')\n","\n","            # Load super resolution images\n","            with open('esrgan_train_imgs.pkl', 'rb') as f:\n","                data = pickle.load(f)\n","\n","            # Extract X_hr and y_train from the loaded data\n","            self.X_train = data['X']\n","            self.y_train = data['y']\n","\n","        # Calculate class weights using class frequency\n","        self.weight = dict(enumerate(compute_class_weight('balanced', classes=np.unique(self.y_train), y=self.y_train)))\n","\n","        # Preprocess train data\n","        self.X_train, self.y_train = self._preprocess_data(self.X_train, self.y_train, augmentation=self.augmentation)\n","\n","        # Preprocess validation data\n","        self.X_validate, self.y_validate = self._preprocess_data(self.X_validate, self.y_validate)\n","\n","        # Get trained history, and trained model\n","        history, classifier = self._train(**kwargs)\n","\n","        # Evaluate model's performance on train and validation set\n","        _evaluate(classifier, history.history, self.X_train, self.y_train, self.X_validate, self.y_validate)\n","\n","def _build_model(hp, base_model, input_size, finetune):\n","        \"\"\"\n","        Builds a customized CNN model for hyperparameter tuning\n","\n","        Params:\n","          - hp: Hyperparameter tuning object\n","          - base_model: Pretrained base model\n","          - input_size: Tuple specifying the input size of the model\n","          - finetune: Boolean indicating whether to finetune the base model\n","\n","        Returns:\n","          - Compiled Keras model\n","        \"\"\"\n","\n","        # Load the base model with specified parameters\n","        base = base_model(include_top=False,\n","                              weights=\"imagenet\",\n","                              input_tensor=K.Input(shape=input_size + (3,)),\n","                              pooling='avg')\n","\n","        # Set whether to fine-tune the base model\n","        base.trainable = finetune\n","\n","        # Define the top layers of the model\n","        x = base.output\n","        x = Dense(3, activation='softmax')(x)\n","        model = Model(inputs=base.input, outputs=x)\n","\n","        # Choose optimizer, learning rate, and weight decay using hyperparameters\n","        optimizer_choice = hp.Choice('optimizer', ['adam', 'sgd', 'rmsprop'])\n","        learning_rate = hp.Choice('learning_rate', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","        weight_decay = hp.Choice('weight_decay', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n","\n","        # Configure optimizer based on the choice\n","        if optimizer_choice == 'adam':\n","            optimizer = Adam(learning_rate=learning_rate, weight_decay=weight_decay)\n","        elif optimizer_choice == 'sgd':\n","            optimizer = SGD(learning_rate=learning_rate, weight_decay=weight_decay)\n","        else:\n","            optimizer = RMSprop(learning_rate=learning_rate, weight_decay=weight_decay)\n","\n","\n","        # Compile the model with the selected optimizer, loss function\n","        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","        return model"],"metadata":{"id":"9wMkey79ECDv"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1F7bs--OLiRBBjwLR7z3BW0NdPwmMdZ1G","timestamp":1711933824087},{"file_id":"1KfKHn9qhA-J_Y8aRUZDAZIT-v-1H2Jvo","timestamp":1710795698552},{"file_id":"1JK51pLhfItCJm3E2c8NGwLnXK1ivz5o0","timestamp":1710771217035}],"collapsed_sections":["2-dUN_5b1bK5","SKzqsMASDkNH","r-Ax59YTCeuj"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}